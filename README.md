# Paper Review 
* **개인 공부**라 열심히는 하고 있으나, 완벽한 리뷰가 아닙니다. 
* 리뷰가 끝나더라도 계속 의문/생각/교정/좋은자료가 있다면 꾸준히 업데이트 됩니다.
* link review는 다른 분들이 하신 좋은 리뷰를 링크한 것입니다.
* lihgt_link는 빠르게 개념(abstract)정도로 본 논문을 의미.
* 현재 상황이 리뷰 공개를 못하고 있는 상황이라, 논문 링크로만 정리진행합니다.

## Deep Learning
* Revisiting Small Batch Training for Deep Neural Networks : [[paper]](https://arxiv.org/abs/1804.07612)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/12) 
* Weight Standardization : [[paper]](https://arxiv.org/abs/1903.10520)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/140) 
* Effects of Image Size on Deep Learning : [[paper]](https://arxiv.org/abs/2101.11508)
* Inductive Bias : [[link_review]](https://velog.io/@euisuk-chung/Inductive-Bias%EB%9E%80) 

## Multi-Label Image Recognition
* Learning Discriminative Representations for Multi-Label Image Recognition : [[paper]](https://arxiv.org/abs/2107.11159)

## Knowledge distillation
* Knowledge distillation: A good teacher is patient and consistent : [[paper]](https://arxiv.org/abs/2106.05237)
* Hierarchical Self-supervised Augmented Knowledge Distillation : [[paper]](https://arxiv.org/abs/2107.13715)
* Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation : [[paper]](https://arxiv.org/abs/2107.12087)

## CLIP & joined multi-modal
* How Much Can CLIP Benefit Vision-and-Language Tasks? : [[paper]](https://arxiv.org/abs/2107.12087)


## Self Supervised Learninig & unsupervised learning
* Unsupervised Representation Learning by Predicting Image Rotations  : [[paper]](https://arxiv.org/abs/1803.07728)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/393)
* Unsupervised Visual Representation Learning by Context Prediction : [[paper]](https://arxiv.org/abs/1505.05192)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/394)
* Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles : [[paper]](https://arxiv.org/abs/1603.09246)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/395)
* Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks : [[paper]](https://arxiv.org/abs/1406.6909)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/396)
* Rethinking Pre-training and Self-training : [[paper]](https://arxiv.org/abs/2006.06)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/397)
* Selfie: Self-supervised Pretraining for Image Embedding : [[paper]](https://arxiv.org/abs/1906.02940) [[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/384)
* Self-training with Noisy Student improves ImageNet classification : [[paper]](https://arxiv.org/abs/1911.04252) [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/246)
* SimCLR : A Simple Framework for Contrastive Learning of Visual Representations : [[paper]](https://arxiv.org/abs/2002.05709)
* MoCo : Momentum Contrast for Unsupervised Visual Representation Learning : [[paper]](https://arxiv.org/abs/1911.05722) 
* MoCo V2 : Improved Baselines with Momentum Contrastive Learning : [[paper]](https://arxiv.org/abs/2003.04297) [[link_review]](https://deep-learning-study.tistory.com/743) [[link_review]](https://hongl.tistory.com/127) 
* MoCo V3 : An Empirical Study of Training Self-Supervised Vision Transformers: [[paper]](https://arxiv.org/abs/2104.02057) [[link_review]](https://deep-learning-study.tistory.com/746)  [[link_review]](https://rauleun.github.io/MoCo_v3)

## Vision Transformers  classification

* Stand-Alone Self-Attention in Vision Models : [[paper]](https://arxiv.org/abs/1906.05909)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/154) [[link_review]](https://hulk89.github.io/machine%20learning/2019/09/10/Stand-Alone-Self-Attention-In-Vision-Models/) [[link_review]](https://hulk89.github.io/machine%20learning/2019/09/10/Stand-Alone-Self-Attention-In-Vision-Models/) [[link_review]](https://cool24151.tistory.com/99) [[link_review]](http://dsba.korea.ac.kr/seminar/?mod=document&uid=31) [[link_review]](https://dreambreaker-ds.tistory.com/entry/Stand-Alone-Self-Attention-in-Visual-Models) [[link_review]](https://ko-kr.facebook.com/groups/TensorFlowKR/permalink/930891160585276/) [[link_review]](https://comlini8-8.tistory.com/37)
* Selfie: Self-supervised Pretraining for Image Embedding : [[paper]](https://arxiv.org/abs/1906.02940) [[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/384)  [[link_review]](https://flonelin.wordpress.com/2019/09/12/stand-alone-self-attention-in-vision/)  [[link_review]](http://aidev.co.kr/deeplearning/7769) 
* ViT:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: [[paper]](https://arxiv.org/abs/2010.11929) [[link_review]](https://jeonsworld.github.io/vision/vit/) [[link_review]](https://eehoeskrap.tistory.com/486) [[link_review]](https://littlefoxdiary.tistory.com/70) [[link_review]](https://littlefoxdiary.tistory.com/70) [[link_review]](https://simonezz.tistory.com/102) [[link_review]](https://deep-learning-study.tistory.com/716) [[link_review]](https://ml.starall.kr/m/25) [[link_review]](https://ai-information.blogspot.com/2021/06/cv-005-image-is-worth-16x16-words.html)
* DeiT:Training data-efficient image transformers & distillation through attention : [[paper]](https://arxiv.org/abs/2012.12877) [[link_review]](https://housekdk.gitbook.io/ml/ml/computer-vision-transformer-based/deit-training-data-efficient-image-transformers-and-distillation-through-attention) [[link_review]](https://sungminlee0810.github.io/paper_review/classification/distillation/tech-post/) [[link_review]](https://deep-learning-study.tistory.com/806) [[link_review]](https://visionhong.tistory.com/29)
* Bottleneck Transformers for Visual Recognition: [[paper]](https://arxiv.org/abs/2101.11605) [[link_review]](https://www.facebook.com/groups/TensorFlowKR/permalink/1424356407905413)
* Going deeper with Image Transformers: [[paper]](https://arxiv.org/abs/2103.17239)
* Rethinking Spatial Dimensions of Vision Transformers : [[paper]](https://arxiv.org/abs/2103.16302)
* On the Adversarial Robustness of Visual Transformers: [[paper]](https://arxiv.org/abs/2103.15670)
* TransFG: A Transformer Architecture for Fine-grained Recognition : [[paper]](https://arxiv.org/abs/2103.07976)
* Understanding Robustness of Transformers for Image Classification : [[paper]](https://arxiv.org/abs/2103.14586)
* DeepViT: Towards Deeper Vision Transformer : [[paper]](https://arxiv.org/abs/2103.11886)
* CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification : [[paper]](https://arxiv.org/abs/2103.14899)
* CvT: Introducing Convolutions to Vision Transformers: [[paper]](https://arxiv.org/abs/2103.15808)  [[link_review]](https://deep-learning-study.tistory.com/816) 
* Efficient Feature Transformations for Discriminative and Generative Continual Learning : [[paper]](https://arxiv.org/abs/2103.13558)
* Swin Transformer: Hierarchical Vision Transformer using Shifted Windows : [[paper]](https://arxiv.org/abs/2103.14030) [[link_review]](https://deep-learning-study.tistory.com/728) [[link_review]](http://dsba.korea.ac.kr/seminar/?mod=document&uid=1793) 
* Can Vision Transformers Learn without Natural Images?: [[paper]](https://arxiv.org/abs/2103.13023)
* Scaling Local Self-Attention for Parameter Efficient Visual Backbones: [[paper]](https://arxiv.org/abs/2103.12731)
* Incorporating Convolution Designs into Visual Transformers : [[paper]](https://arxiv.org/abs/2103.11816)
* ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases : [[paper]](https://arxiv.org/abs/2103.10697)
* Explicitly Modeled Attention Maps for Image Classification : [[paper]](https://arxiv.org/abs/2006.07872)
* Conditional Positional Encodings for Vision Transformers : [[paper]](https://arxiv.org/abs/2102.10882)
* Transformer in Transformer: [[paper]](https://arxiv.org/abs/2103.00112) [[link_review]](https://medium.com/@nabil.madali/transformer-in-transformer-a36d1a7845b8) 
* A Survey on Visual Transformer: [[paper]](https://arxiv.org/abs/2012.12556)
* Co-Scale Conv-Attentional Image Transformers: [[paper]](https://arxiv.org/abs/2104.06399)
* Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity : [[paper]](https://arxiv.org/abs/2101.03961) [[link_review]](https://www.slideshare.net/hoondori97/switch-transformers-paper-review) 
* LocalViT: Bringing Locality to Vision Transformers : [[paper]](https://arxiv.org/abs/2104.05707)
* Visformer: The Vision-friendly Transformer : [[paper]](https://arxiv.org/pdf/2104.12533.pdf)
* Multiscale Vision Transformers : [[paper]](https://arxiv.org/abs/2104.11227) [[link_review]](https://www.marktechpost.com/2021/08/13/facebook-ai-introduces-multiscale-vision-transformers-mvit-a-transformer-architecture-for-representation-learning-from-visual-data/)  [[link_review]](https://hugrypiggykim.com/2021/08/08/multiscale-vision-transformersmvit/)
* So-ViT: Mind Visual Tokens for Vision Transformer: [[paper]](https://arxiv.org/abs/2104.10935)
* Token Labeling: Training a 85.4% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet (이후 "All Tokens Matter: Token Labeling for Training Better Vision Transformers 변경"): [[paper]](https://arxiv.org/abs/2104.10858)
* Fourier Image Transformer: [[paper]](https://arxiv.org/abs/2104.02555)
* Emerging Properties in Self-Supervised Vision Transformers: [[paper]](https://arxiv.org/abs/2104.14294)
* ConTNet: Why not use convolution and transformer at the same time?: [[paper]](https://arxiv.org/abs/2104.13497)
* Twins: Revisiting Spatial Attention Design in Vision Transformers: [[paper]](https://arxiv.org/abs/2104.13840)
* MoCo V3 :An Empirical Study of Training Self-Supervised Vision Transformers: [[paper]](https://arxiv.org/abs/2104.02057) [[link_review]](https://deep-learning-study.tistory.com/746)  [[link_review]](https://rauleun.github.io/MoCo_v3)
* Conformer: Local Features Coupling Global Representations for Visual Recognition: [[paper]](https://arxiv.org/abs/2105.03889)
* Self-Supervised Learning with Swin Transformers: [[paper]](https://arxiv.org/abs/2105.04553)
* Are Pre-trained Convolutions Better than Pre-trained Transformers?: [[paper]](https://arxiv.org/abs/2105.03322)
* LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference: [[paper]](https://arxiv.org/abs/2104.01136)
* Are Convolutional Neural Networks or Transformers more like human vision?: [[paper]](https://arxiv.org/abs/2105.07197)
* Rethinking Skip Connection with Layer Normalization in Transformers and ResNets: [[paper]](https://arxiv.org/abs/2105.07205)
* Rethinking the Design Principles of Robust Vision Transformer: [[paper]](https://arxiv.org/abs/2105.07926)
* Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding: [[paper]](https://arxiv.org/abs/2103.15358)
* On the Robustness of Vision Transformers to Adversarial Examples: [[paper]](https://arxiv.org/abs/2104.02610)
* Refiner: Refining Self-attention for Vision Transformers: [[paper]](https://arxiv.org/abs/2106.03714)
* Patch Slimming for Efficient Vision Transformers: [[paper]](https://arxiv.org/abs/2106.02852)
* CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings: [[paper]](https://arxiv.org/abs/2106.03143)
* RegionViT: Regional-to-Local Attention for Vision Transformers: [[paper]](https://arxiv.org/abs/2106.02689)
* X-volution: On the unification of convolution and self-attention: [[paper]](https://arxiv.org/abs/2106.02253)
* The Image Local Autoregressive Transformer: [[paper]](https://arxiv.org/abs/2106.02514)
* Glance-and-Gaze Vision Transformer: [[paper]](https://arxiv.org/abs/2106.02277)
* Semantic Correspondence with Transformers: [[paper]](https://arxiv.org/abs/2106.02520)
* DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification: [[paper]](https://arxiv.org/abs/2106.02034)
* When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations: [[paper]](https://arxiv.org/abs/2106.01548)
* KVT: k-NN Attention for Boosting Vision Transformers: [[paper]](https://arxiv.org/abs/2106.00515)
* Less is More: Pay Less Attention in Vision Transformers: [[paper]](https://arxiv.org/abs/2105.14217)
* FoveaTer: Foveated Transformer for Image Classification: [[paper]](https://arxiv.org/abs/2105.14173)
* An Attention Free Transformer: [[paper]](https://arxiv.org/abs/2105.14103)
* Optimizing Deeper Transformers on Small Datasets: [[paper]](https://arxiv.org/abs/2012.15355)
* Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length: [[paper]](https://arxiv.org/abs/2105.15075)
* Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks: [[paper]](https://arxiv.org/abs/2105.02358)
* Pre-Trained Image Processing Transformer: [[paper]](https://arxiv.org/abs/2012.00364)
* ResT: An Efficient Transformer for Visual Recognition: [[paper]](https://arxiv.org/abs/2105.13677)
* Towards Robust Vision Transformer: [[paper]](https://arxiv.org/abs/2105.07926)
* Aggregating Nested Transformers: [[paper]](https://arxiv.org/abs/2105.12723)
* GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification: [[paper]](https://arxiv.org/abs/2104.14528)
* Emerging Properties in Self-Supervised Vision Transformers: [[paper]](https://arxiv.org/abs/2104.14294)
* Intriguing Properties of Vision Transformers: [[paper]](https://arxiv.org/abs/2105.10497)
* Combining Transformer Generators with Convolutional Discriminators: [[paper]](https://arxiv.org/abs/2105.10189)
* Relative Positional Encoding for Transformers with Linear Complexity: [[paper]](https://arxiv.org/abs/2105.08399)
* Vision Transformers are Robust Learners: [[paper]](https://arxiv.org/abs/2105.07581)
* Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer: [[paper]](https://arxiv.org/abs/2106.03650)
* A Survey of Transformers: [[paper]](https://arxiv.org/abs/2106.04554)
* Armour: Generalizable Compact Self-Attention for Vision Transformers : [[paper]](https://arxiv.org/abs/2108.01778)
* Vision Transformer with Progressive Sampling : [[paper]](https://arxiv.org/abs/2108.01684)
* Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer : [[paper]](https://arxiv.org/abs/2108.01390)
* DPT: Deformable Patch-based Transformer for Visual Recognition : [[paper]](https://arxiv.org/abs/2107.14467)
* CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings : [[paper]](https://arxiv.org/abs/2106.03143) 
* Rethinking and Improving Relative Position Encoding for Vision Transformer : [[paper]](https://arxiv.org/abs/2107.14222) 
* Dual-stream Network for Visual Recognition : [[paper]](https://arxiv.org/abs/2105.14734) 
* BEiT: BERT Pre-Training of Image Transformers : [[paper]](https://arxiv.org/abs/2106.08254) 
* Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions : [[paper]](https://arxiv.org/abs/2102.12122) 
* PVTv2: Improved Baselines with Pyramid Vision Transformer : [[paper]](https://arxiv.org/abs/2106.13797) 
* Thinking Like Transformers : [[paper]](https://arxiv.org/abs/2106.06981) 
* CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows : [[paper]](https://arxiv.org/abs/2107.00652) 
* CMT: Convolutional Neural Networks Meet Vision Transformers : [[paper]](https://arxiv.org/abs/2107.06263) 
* Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition : [[paper]](https://arxiv.org/abs/2107.06538) 
* ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias : [[paper]](https://arxiv.org/abs/2106.03348) 
* Visual Transformer Pruning : [[paper]](https://arxiv.org/abs/2104.08500) 
* Rethinking Positional Encoding : [[paper]](https://arxiv.org/abs/2107.02561) 
* Local-to-Global Self-Attention in Vision Transformers : [[paper]](https://arxiv.org/abs/2107.04735) 
* ResT: An Efficient Transformer for Visual Recognition : [[paper]](https://arxiv.org/abs/2105.13677) 
* Feature Fusion Vision Transformer for Fine-Grained Visual Categorization : [[paper]](https://arxiv.org/abs/2107.02341) 
* Vision Xformers: Efficient Attention for Image Classification : [[paper]](https://arxiv.org/abs/2107.02239) 
* EsViT : Efficient Self-supervised Vision Transformers for Representation Learning : [[paper]](https://arxiv.org/abs/2106.09785) 
* GLiT: Neural Architecture Search for Global and Local Image Transformer : [[paper]](https://arxiv.org/abs/2107.02960) 
* Efficient Vision Transformers via Fine-Grained Manifold Distillation : [[paper]](https://arxiv.org/abs/2107.01378) 
* What Makes for Hierarchical Vision Transformer? : [[paper]](https://arxiv.org/abs/2107.02174) 
* AutoFormer: Searching Transformers for Visual Recognition : [[paper]](https://arxiv.org/abs/2107.00651) 
* Are Convolutional Neural Networks or Transformers more like human vision? : [[paper]](https://arxiv.org/abs/2105.07197) 
* Focal Self-attention for Local-Global Interactions in Vision Transformers : [[paper]](https://arxiv.org/abs/2107.00641) 


## Vision Transformers vs MLP
* AS-MLP: An Axial Shifted MLP Architecture for Vision : [[paper]](https://arxiv.org/abs/2107.08391) 
* S2-MLPv2: Improved Spatial-Shift MLP Architecture for Vision : [[paper]](https://arxiv.org/abs/2108.01072)
* ResMLP: Feedforward networks for image classification with data-efficient training: [[paper]](https://arxiv.org/abs/2105.03404)
* Pay Attention to MLPs: [[paper]](https://arxiv.org/abs/2105.08050)
* Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet: [[paper]](https://arxiv.org/abs/2105.02723)
* MLP-Mixer: An all-MLP Architecture for Vision : [[paper]](https://arxiv.org/abs/2105.01601)


## Vision Transformers retrieval 
* Investigating the Vision Transformer Model for Image Retrieval Tasks: [[paper]](https://arxiv.org/abs/2101.03771)
* Training Vision Transformers for Image Retrieval: [[paper]](https://arxiv.org/abs/2102.05644)
* Instance-level Image Retrieval using Reranking Transformers: [[paper]](https://arxiv.org/abs/2103.12236)
* Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for Improved Cross-Modal Retrieval: [[paper]](https://arxiv.org/abs/2103.11920)
* TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval : [[paper]](https://arxiv.org/abs/2105.01823)

## Vision Transformers segmentation and detection
* CoSformer: Detecting Co-Salient Object with Transformers: [[paper]](https://arxiv.org/abs/2104.14729)
* MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding: [[paper]](https://arxiv.org/abs/2104.12763)
* Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks: [[paper]](https://arxiv.org/abs/2105.02358)
* Medical Image Segmentation Using Squeeze-and-Expansion Transformers: [[paper]](https://arxiv.org/abs/2105.09511)
* SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers: [[paper]](https://arxiv.org/abs/2105.15203)
* Visual Transformers: Token-based Image Representation and Processing for Computer Vision : [[paper]](https://arxiv.org/abs/2006.03677)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/388)
* DETR:End-to-End Object Detection with Transformers : [[paper]](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers) [[link_review]](https://amaarora.github.io/2021/07/26/annotateddetr.html) [[link_review]](https://keyog.tistory.com/32) [[link_review]](https://powerofsummary.tistory.com/205) [[link_review]](https://modulabs.oopy.io/38070399-f3af-409f-92b0-b640ab654f53)
* Unifying Global-Local Representations in Salient Object Detection with Transformer : [[paper]](https://arxiv.org/abs/2108.02759)
* A Unified Efficient Pyramid Transformer for Semantic Segmentation : [[paper]](https://arxiv.org/abs/2107.14209)
* Dual-stream Network for Visual Recognition : [[paper]](https://arxiv.org/abs/2105.14734) 
* MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers : [[paper]](https://arxiv.org/abs/2012.00759) 
* Vision Transformers with Patch Diversification : [[paper]](https://arxiv.org/abs/2104.12753) 
* Improve Vision Transformers Training by Suppressing Over-smoothing : [[paper]](https://arxiv.org/abs/2104.12753v1)


## Vision Transformers video
* An Image is Worth 16x16 Words, What is a Video Worth?: [[paper]](https://arxiv.org/abs/2103.13915)
* Token Shift Transformer for Video Classification : [[paper]](https://arxiv.org/abs/2108.02432)

## Vision Transformers face
* Robust Facial Expression Recognition with Convolutional Visual Transformers : [[paper]](https://arxiv.org/abs/2103.16854)
* Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition : [[paper]](https://arxiv.org/abs/2107.03107)

## Vision Transformers OCR
* NRTR: A No-Recurrence Sequence-to-Sequence Model For Scene Text Recognition : [[paper]](https://arxiv.org/abs/1806.00926)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/221)
* On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention  : [[paper]](https://arxiv.org/abs/1910.04396)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/220)
* 2D Attentional Irregular Scene Text Recognizer : [[paper]](https://arxiv.org/abs/1906.05708)[[]](https://github.com/chullhwan-song/Reading-Paper/issues/389)

## Vision Transformers multi-modal
* Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretrainingr : [[paper]](https://arxiv.org/abs/2107.14572)
* ReFormer: The Relational Transformer for Image Captioning : [[paper]](https://arxiv.org/abs/2107.14178)
* Long-Short Transformer: Efficient Transformers for Language and Vision : [[paper]](https://arxiv.org/abs/2107.02192)

## Vision Transformers GAN
* A Hierarchical Transformation-Discriminating Generative Model for Few Shot Anomaly Detection : [[paper]](https://arxiv.org/abs/2104.14535)
* ViTGAN: Training GANs with Vision Transformers : [[paper]](https://arxiv.org/abs/2107.04589)
* Styleformer: Transformer based Generative Adversarial Networks with Style Vector : [[paper]](https://arxiv.org/abs/2106.07023)




## [Image Retrieval](https://github.com/chullhwan-song/Image-Retrieval) (Instance level Image Retrieval) & Deep Feature 
* Large-Scale Image Retrieval with Attentive Deep Local Features : [[paper]](https://arxiv.org/abs/1612.06321)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/4)
* NetVLAD: CNN architecture for weakly supervised place recognition : [[paper]](https://arxiv.org/abs/1511.07247)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/3)
* Learning visual similarity for product design with convolutional neural networks : [[paper]](https://www.cs.cornell.edu/~kb/publications/SIG15ProductNet.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/7) 
* Bags of Local Convolutional Features for Scalable Instance Search  : [[paper]](https://arxiv.org/abs/1604.04653)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/9) 
* Neural Codes for Image Retrieval : [[paper]](https://arxiv.org/abs/1404.1777)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/14) 
* Conditional Similarity Networks : [[paper]](https://arxiv.org/abs/1603.07810)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/15)
* End-to-end Learning of Deep Visual Representations for Image Retrieval : [[paper]](https://arxiv.org/abs/1610.07940)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/17)
* CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples : [[paper]](https://arxiv.org/abs/1604.02426)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/19)
* Image similarity using Deep CNN and Curriculum Learning : [[paper]](https://arxiv.org/abs/1709.08761)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/29)
* Faster R-CNN Features for Instance Search : [[paper]](http://arxiv.org/abs/1604.08893)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/37)
* Regional Attention Based Deep Feature for Image Retrieval : [[paper]](https://sglab.kaist.ac.kr/RegionalAttention/)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/40)
* Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination : [[paper]](https://arxiv.org/abs/1805.01978)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/47)
* Object retrieval with deep convolutional features : [[paper]](http://doras.dcu.ie/22134/1/ios-press-object.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/49)
* Cross-dimensional Weighting for Aggregated Deep Convolutional Features : [[paper]](https://arxiv.org/abs/1512.04065)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/51)
* Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling : [[paper]](https://arxiv.org/abs/1810.04652)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/55)
* Saliency Weighted Convolutional Features for Instance Search : [[paper]](https://arxiv.org/abs/1711.10795)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/103)
* 2018 Google Landmark Retrieval Challenge 리뷰 : [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/105)
* 2019 Google Landmark Retrieval Challenge 리뷰 : [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/137)
* REMAP: Multi-layer entropy-guided pooling of dense CNN features for image retrieval : [[paper]](https://arxiv.org/abs/1906.06626)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/145)
* Large-scale Landmark Retrieval/Recognition under a Noisy and Diverse Dataset : [[paper]](https://arxiv.org/abs/1906.04087)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/143)
* Fine-tuning CNN Image Retrieval with No Human Annotation : [[paper]](https://arxiv.org/abs/1711.02512)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/153)
* Large Scale Landmark Recognition via Deep Metric Learning : [[paper]](https://arxiv.org/abs/1908.10192)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/186)
* Deep Aggregation of Regional Convolutional Activations for Content Based Image Retrieval : [[paper]](https://arxiv.org/abs/1909.09420)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/205)
* Challenging deep image descriptors for retrieval in heterogeneous iconographic collections : [[paper]](https://arxiv.org/abs/1909.08866v1)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/207)
* A Benchmark on Tricks for Large-scale Image Retrieval : [[paper]](https://arxiv.org/abs/1907.11854)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/177)
* Attention-Aware Generalized Mean Pooling for Image Retrieval : [[paper]](https://arxiv.org/abs/1811.00202)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/157)
* Class-Weighted Convolutional Features for Image Retrieval : [[paper]](https://arxiv.org/abs/1707.02581)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/231) # 100th
* deep image retrieval loss (계속 업데이트):[[paper]](https://github.com/chullhwan-song/Reading-Paper/issues/148)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/181)
* Matchable Image Retrieval by Learning from Surface Reconstruction:[[paper]](https://arxiv.org/abs/1811.10343)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/269)
* Regional Maximum Activations of Convolutions with Attention for Cross-domain Beauty and Personal Care Product Retrieval:[[paper]](https://dl.acm.org/citation.cfm?id=3266436)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/270)
* Combination of Multiple Global Descriptors for Image Retrieval:[[paper]](https://arxiv.org/abs/1903.10663)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/313)
* Unifying Deep Local and Global Features for Efficient Image Search:[[paper]](https://arxiv.org/abs/2001.05027)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/290)
* ACTNET: end-to-end learning of feature activations and multi-stream aggregation for effective instance image retrieval:[[paper]](https://arxiv.org/abs/1907.05794)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/351)
* Google Landmarks Dataset v2 A Large-Scale Benchmark for Instance-Level Recognition and Retrieval:[[paper]](https://arxiv.org/abs/2004.01804)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/358)
* Detect-to-Retrieve: Efficient Regional Aggregation for Image Search:[[paper]](https://arxiv.org/abs/1812.01584)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/141)
* Local Features and Visual Words Emerge in Activations:[[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Simeoni_Local_Features_and_Visual_Words_Emerge_in_Activations_CVPR_2019_paper.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/150)
* Image Retrieval using Multi-scale CNN Features Pooling: [[paper]](https://arxiv.org/abs/2004.09695)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/372)
* MultiGrain: a unified image embedding for classes and instances: [[paper]](https://arxiv.org/abs/1902.05509)[[link_review]](https://norman3.github.io/papers/docs/multigrain) [[link_review]](https://github-wiki-see.page/m/penny4860/study-note/wiki/%5B%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0%5D-MultiGrain-%3A-a-unified-image-embedding-for-classes-and-instances)
* Divide and Conquer the Embedding Space for Metric Learning: [[paper]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.pdf)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/387)
* An Effective Pipeline for a Real-world Clothes Retrieval System: [[paper]](https://arxiv.org/abs/2005.12739)[[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/386)
* Instance Similarity Learning for Unsupervised Feature Representation : [[paper]](https://arxiv.org/abs/2108.02721)
* Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretrainingr : [[paper]](https://arxiv.org/abs/2107.14572)
* eProduct: A Million-Scale Visual Search Benchmark to Address Product Recognition Challenges : [[paper]](https://arxiv.org/abs/2107.05856)
* Towards Accurate Localization by Instance Search : [[paper]](https://arxiv.org/abs/2107.05005)
* The 2021 Image Similarity Dataset and Challenge : [[paper]](https://arxiv.org/abs/2106.09672)


## Metric Learning
* Deep metric learning using Triplet network : [[paper]](https://arxiv.org/abs/1412.6622)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/16)
* FaceNet: A Unified Embedding for Face Recognition and Clustering : [[paper]](https://arxiv.org/abs/1503.03832)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/43)
* Sampling Matters in Deep Embedding Learning : [[paper]](https://arxiv.org/abs/1706.07567)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/324)
* Online Progressive Deep Metric Learning : [[paper]](https://arxiv.org/abs/1805.05510) 


## Fashion Image Retrieval
* Learning Embeddings for Product Visual Search with Triplet Loss and Online Sampling : [[paper]](https://arxiv.org/abs/1810.04652)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/55)
* Conditional Similarity Networks : [[paper]](https://arxiv.org/abs/1603.07810)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/15)

## Fashion Recommendation
* FashionNet: Personalized Outfit Recommendation with Deep Neural Network: [[paper]](https://arxiv.org/abs/1612.06321)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/56)
* Context-Aware Visual Compatibility Prediction: [[paper]](https://arxiv.org/abs/1902.03646)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/109)
* Learning Type-Aware Embeddings for Fashion Compatibility : [[paper]](https://arxiv.org/abs/1902.03646)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/360)

## Fashion Generative Adversarial Nets
* Be Your Own Prada: Fashion Synthesis with Structural Coherence : [[paper]](http://mmlab.ie.cuhk.edu.hk/projects/FashionGAN/)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/92)
* Fashion-Gen: The Generative Fashion Dataset and Challenge : [[paper]](https://arxiv.org/abs/1806.08317)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/95)
* DwNet: Dense warp-based network for pose-guided human video generation: [[paper]](https://arxiv.org/abs/1910.09139)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/233)


## Image Retrieval using Deep Hash
* Deep Learning of Binary Hash Codes for Fast Image Retrieval : [[paper]](https://www.iis.sinica.edu.tw/~kevinlin311.tw/cvprw15.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/78)
* Feature Learning based Deep Supervised Hashing with Pairwise Labels : [[paper]](https://arxiv.org/abs/1511.03855)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/79)
* Deep Supervised Hashing with Triplet Labels : [[paper]](https://arxiv.org/abs/1612.03900)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/86)
* Online Hashing with Similarity Learning : [[paper]](https://arxiv.org/abs/2108.02560)


## Video Classification
* NetVLAD: CNN architecture for weakly supervised place recognition : [[paper]](https://arxiv.org/abs/1511.07247)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/3)
* Learnable pooling with Context Gating for video classification : [[paper]](https://arxiv.org/abs/1706.06905)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/30)
* Less is More: Learning Highlight Detection from Video Duration : [[paper]](https://arxiv.org/abs/1903.00859)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/113)
* Efficient Video Classification Using Fewer Frames : [[paper]](https://arxiv.org/abs/1902.10640)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/110)

## [OCR](https://github.com/chullhwan-song/OCR) - Recognition 
* Synthetically Supervised Feature Learning for Scene Text Recognition : [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Liu_Synthetically_Supervised_Feature_ECCV_2018_paper.html)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/60)
* FOTS: Fast Oriented Text Spotting with a Unified Network : [[paper]](https://arxiv.org/abs/1801.01671)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/107)
* Robust Scene Text Recognition with Automatic Rectification : [[paper]](https://arxiv.org/abs/1603.03915)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/124)
* Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition : [[paper]](https://arxiv.org/abs/2107.12090)


## [OCR](https://github.com/chullhwan-song/OCR) - Detection 
* PixelLink: Detecting Scene Text via Instance Segmentation : [[paper]](https://arxiv.org/abs/1801.01315)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/71)
* EAST: An Efficient and Accurate Scene Text Detector : [[paper]](https://arxiv.org/abs/1704.03155)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/73)
* Scene Text Detection with Supervised Pyramid Context Network : [[paper]](https://arxiv.org/abs/1811.08605)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/75)
* FOTS: Fast Oriented Text Spotting with a Unified Network : [[paper]](https://arxiv.org/abs/1801.01671)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/107)
* Character Region Awareness for Text Detection : [[paper]](https://arxiv.org/abs/1904.01941)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/136)

## Attention & Deformation
* Squeeze Excitation Networks : [[paper]](https://arxiv.org/abs/1709.01507)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/38)
* Spatial Transformer Network : [[paper]](https://arxiv.org/abs/1506.02025)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/2)
* Tell Me Where to Look: Guided Attention Inference Network : [[paper]](https://arxiv.org/abs/1802.10171)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/11)
* CBAM: Convolutional Block Attention Module : [[paper]](https://arxiv.org/abs/1807.06521)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/24)
* BAM: Bottleneck Attention Module : [[paper]](https://arxiv.org/abs/1807.06514)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/32)
* Neural Machine Translation by Jointly Learning to Align and Translate : [[paper]](https://arxiv.org/abs/1409.0473)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/32)
* Residual Attention Networks for Image Classification : [[paper]](https://arxiv.org/abs/1704.06904)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/35)
* Attention is all you need  : [[paper]](https://arxiv.org/abs/1706.03762)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/54)[[link_review]](https://pozalabs.github.io/transformer/)
* Residual Attention Network for Image Classification : [[paper]](https://arxiv.org/abs/1704.06904)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/102)
* Stand-Alone Self-Attention in Vision Models : [[paper]](https://arxiv.org/abs/1906.05909)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/154)

## Visual & Textual Embedding
* DeViSE: A Deep Visual-Semantic Embedding Model : [[paper]](https://research.google.com/pubs/pub41869.html)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/1)
* Dual Attention Networks for Multimodal Reasoning and Matching : [[paper]](https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwiOl5Pj19LUAhVKvLwKHVpoDdcQFggvMAE&url=https%3A%2F%2Farxiv.org%2Fpdf%2F1611.00471&usg=AFQjCNEkNnTcTYyq7AI9uFuQKDHom0ai1w)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/21)
* Learning Deep Structure-Preserving Image-Text Embeddings : [[paper]](https://arxiv.org/abs/1511.06078)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/26)
* Learning Two-Branch Neural Networks for Image-Text Matching Tasks : [[paper]](https://arxiv.org/abs/1810.02443)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/56)

## Recommendation
* FashionNet: Personalized Outfit Recommendation with Deep Neural Network: [[paper]](https://arxiv.org/abs/1612.06321)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/56)
* Context-Aware Visual Compatibility Prediction: [[paper]](https://arxiv.org/abs/1902.03646)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/109)

## CNN
* Imagenet classification with deep convolutional neural networks : [[paper]](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/6)
* Going Deeper with Convolutions  : [[paper]](https://arxiv.org/abs/1409.4842)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/13)
* ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices : [[paper]](https://arxiv.org/abs/1707.01083)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/18)
* Deep Residual Learning for Image Recognition : [[paper]](https://arxiv.org/abs/1512.03385%5C)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/23)
* Aggregated Residual Transformations for Deep Neural Networks : [[paper]](https://github.com/facebookresearch/ResNeXt)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/28)
* Very Deep Convolutional Networks for Large-Scale Image Recognition : [[paper]](https://arxiv.org/abs/1409.1556)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/33)
* Squeeze Excitation Networks : [[paper]](https://arxiv.org/abs/1709.01507)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/38)
* MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications : [[paper]](https://arxiv.org/abs/1704.04861)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/44)
* Pelee: A Real-Time Object Detection System on Mobile Devices : [[paper]](https://arxiv.org/abs/1804.06882)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/67)
* Residual Attention Network for Image Classification : [[paper]](https://arxiv.org/abs/1704.06904)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/102)
* Wide Residual Networks : [[paper]](https://arxiv.org/abs/1605.07146)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/88)
* Stand-Alone Self-Attention in Vision Models : [[paper]](https://arxiv.org/abs/1906.05909)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/154)
* Selective Kernel Networks : [[paper]](https://arxiv.org/abs/1903.06586)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/232)
* EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks : [[paper]](https://arxiv.org/abs/1905.11946) [[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/255)
* Self-training with Noisy Student improves ImageNet classification : [[paper]](https://arxiv.org/abs/1911.04252) [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/246)
* Selfie: Self-supervised Pretraining for Image Embedding : [[paper]](https://arxiv.org/abs/1906.02940) [[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/384)

## Transfer Learning
* Taskonomy: Disentangling Task Transfer Learning : [[paper]](https://arxiv.org/abs/1804.08328)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/45)
* What makes ImageNet good for transfer learning?g : [[paper]](https://arxiv.org/abs/1608.08614)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/104)

## Generative Adversarial Nets
* Generative Adversarial Nets : [[paper]](https://arxiv.org/abs/1406.2661)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/8)
* Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks : [[paper]](https://arxiv.org/pdf/1703.10593.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/57)
* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks : [[paper]](https://arxiv.org/abs/1511.06434)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/58)
* Progressive Growing of GANs for Improved Quality, Stability, and Variation : [[paper]](https://arxiv.org/abs/1710.10196)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/76)
* Beholder-GAN: Generation and Beautification of Facial Images with Conditioning on Their Beauty Level : [[paper]](https://arxiv.org/abs/1902.02593)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/81)
* Synthetically Supervised Feature Learning for Scene Text Recognition  : [[paper]](http://openaccess.thecvf.com/content_ECCV_2018/html/Yang_Liu_Synthetically_Supervised_Feature_ECCV_2018_paper.html)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/60)
* A Style-Based Generator Architecture for Generative Adversarial Networks : [[paper]](https://arxiv.org/abs/1812.04948)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/83)
* High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs : [[paper]](https://arxiv.org/abs/1711.11585)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/101)
* Everybody Dance Now : [[paper]](https://arxiv.org/abs/1808.07371)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/100)
* Be Your Own Prada: Fashion Synthesis with Structural Coherence : [[paper]](http://mmlab.ie.cuhk.edu.hk/projects/FashionGAN/)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/92)
* Fashion-Gen: The Generative Fashion Dataset and Challenge : [[paper]](https://arxiv.org/abs/1806.08317)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/95)
* StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks : [[paper]](https://arxiv.org/abs/1710.10916)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/96)
* DwNet: Dense warp-based network for pose-guided human video generation: [[paper]](https://arxiv.org/abs/1910.09139)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/233)

## Face
* FaceNet: A Unified Embedding for Face Recognition and Clustering : [[paper]](https://arxiv.org/abs/1503.03832)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/43)
* The Devil of Face Recognition is in the Noise : [[paper]](https://arxiv.org/abs/1807.11649)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/41)
* Revisiting a single-stage method for face detection : [[paper]](https://arxiv.org/abs/1902.01559)[[review](https://github.com/chullhwan-song/Reading-Paper/issues/77)
* MixFaceNets: Extremely Efficient Face Recognition Networks : [[paper]](https://arxiv.org/abs/2107.13046)


## Pose Estimation
* Convolutional Pose Machines : [[paper]](https://arxiv.org/abs/1602.00134)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/48)

## NLP/NLU
* Efficient Estimation of Word Representations in Vector Space  : [[paper]](https://arxiv.org/abs/1301.3781)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/36)
* node2vec: Scalable Feature Learning for Networks  : [[paper]](https://cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/50)
* Transfomer(self attention) 기본 이해하기 : [PPT정리](https://github.com/chullhwan-song/Reading-Paper/files/3986802/Self.Attention.pptx)
* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding : [[paper]](https://arxiv.org/abs/1810.04805)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/202)(~ing)

## [Learning to Rank](https://github.com/chullhwan-song/Neural-Ranking-Study)
* **DeepRank**: A New Deep Architecture for Relevance Ranking in Information Retrieval : [[paper]](https://arxiv.org/abs/1710.05649)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/52)
* **SNRM**: From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing : [[paper]](https://arxiv.org/abs/1709.05424)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/162)
* **TF-Ranking**: Scalable TensorFlow Library for Learning-to-Rank : [[paper]](https://arxiv.org/abs/1812.00073)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/163)
* **ConvRankNet**: Deep Neural Network for Learning to Rank Query-Text Pairs : [[paper]](https://arxiv.org/abs/1802.08988)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/256)
* **KNRM**: End-to-End Neural Ad-hoc Ranking with Kernel Pooling : [[paper]](https://arxiv.org/abs/1706.06613)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/258)
* **Conv-KNRM**: Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search : [[paper]](http://www.cs.cmu.edu/~./callan/Papers/wsdm18-zhuyun-dai.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/257)
* **PACRR**: A position-aware neural IR model for relevance matching : [[paper]](https://arxiv.org/abs/1704.08803)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/267)
* **CEDR**: Contextualized Embeddings for Document Ranking #262 : [[paper]](https://arxiv.org/abs/1904.07094)[[link]](https://github.com/chullhwan-song/Reading-Paper/issues/262)
* Deeper Text Understanding for IR with Contextual Neural Language Modeling : [[paper]](https://arxiv.org/abs/1905.09217v1)[[link]](https://github.com/chullhwan-song/Reading-Paper/issues/264)
* Simple Applications of BERT for Ad Hoc Document Retrieval : [[paper]](https://arxiv.org/abs/1903.10972)[[link]](https://github.com/chullhwan-song/Reading-Paper/issues/279)
* Document Expansion by Query Prediction : [[paper]](https://arxiv.org/abs/1904.08375)[[link]](https://github.com/chullhwan-song/Reading-Paper/issues/284)
* Passage Re-ranking with BERT : [[paper]](https://arxiv.org/abs/1901.04085)[[link]](https://github.com/chullhwan-song/Reading-Paper/issues/286)

## Domain Adaptation
* Domain-Adversarial Training of Neural Networks : [[paper]](https://arxiv.org/abs/1505.07818)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/31)

## Curriculum Learning
* CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images : [[paper]](https://arxiv.org/abs/1808.01097)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/59)

## Image Segmentation
* U-Net: Convolutional Networks for Biomedical Image Segmentation : [[paper]](https://arxiv.org/abs/1505.04597)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/20)
* Mask R-CNN : [[paper]](https://arxiv.org/abs/1703.06870)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/22)
* Fully Convolutional Networks for Semantic Segmentation : [[paper]](https://arxiv.org/abs/1411.4038)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/74)
* Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation : [[paper]](https://arxiv.org/abs/1901.04949)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/91)
* FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference : [[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/139)

## Localization
* YOLO: Real-Time Object Detection : [[paper]](https://arxiv.org/abs/1506.02640)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/5)
* YOLO9000: Better, Faster, Stronger : [[paper]](https://arxiv.org/abs/1612.08242)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/42)
* Faster R-CNN : [[paper]](https://arxiv.org/abs/1506.01497)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/46)
  * faster rcnn의 anchor generator 개념 뿐만 아니라 소스레벨에서도 이해하기 : [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/184)
* SSD: Single Shot MultiBox Detector : [[paper]](https://arxiv.org/abs/1512.02325)[[link_review]](https://www.youtube.com/watch?v=ej1ISEoAK5g&feature=youtu.be&fbclid=IwAR0hLTV5Vf9giN0utJ_2DtcoBJzkuO91-cTlR1UpPzlp38mD5KAOeqzKxJQ)
  *  Why normalization performed only for conv4_3? : [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/213#issuecomment-538815095)
* Pelee: A Real-Time Object Detection System on Mobile Devices : [[paper]](https://arxiv.org/abs/1804.06882)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/67)
* R-FCN: Object Detection via Region-based Fully Convolutional Networks: [[paper]](https://arxiv.org/abs/1605.06409)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/68)
* Revisiting a single-stage method for face detection: [[paper]](https://arxiv.org/abs/1902.01559)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/77)
* DSSD : Deconvolutional Single Shot Detector: [[paper]](https://arxiv.org/abs/1701.06659)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/62)
* Feature-fused SSD: fast detection for small objects : [[paper]](https://arxiv.org/abs/1709.05054)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/155)
* EfficientDet ： Scalable and Efficient Object Detection :  [[paper]](https://arxiv.org/abs/1911.09070) [[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/253) [[review]](https://github.com/chullhwan-song/Reading-Paper/issues/253)
* FCOS: Fully Convolutional One-Stage Object Detection : [[paper]](https://arxiv.org/abs/1904.01355) [[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/385)
* Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection : [[paper]](https://arxiv.org/abs/1912.02424) [[light_review]](https://github.com/chullhwan-song/Reading-Paper/issues/304)

## AutoML 
* Learning Transferable Architectures for Scalable Image Recognition : [[paper]](https://arxiv.org/abs/1707.07012)[[link_review]](http://research.sualab.com/machine-learning/computer-vision/2018/09/28/nasnet-review.html)

## Image Quality
* Learning to Compose with Professional Photographs on the Web : [[paper]](https://arxiv.org/abs/1702.00503)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/10)
* Photo Aesthetics Ranking Network with Attributes and Content Adaptation : [[paper]](http://users.eecs.northwestern.edu/~xsh835/assets/eccv2016_aesthetics.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/65)
* Composition-preserving Deep Photo Aesthetics Assessment : [[paper]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Mai_Composition-Preserving_Deep_Photo_CVPR_2016_paper.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/66)
* Deep Image Aesthetics Classification using Inception Modules and Fine-tuning Connected Layer : [[paper]](http://jinxin.me/downloads/papers/019-WCSP2016a/ILGNet-Final.pdf)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/99)
* NIMA: Neural Image Assessment : [[paper]](https://arxiv.org/abs/1709.05424)[[review]](https://github.com/chullhwan-song/Reading-Paper/issues/119)

## Others
* Neural Arithmetic Logic Units : [[paper]](https://ciir-publications.cs.umass.edu/pub/web/getpdf.php?id=1302)[[link_review]](https://github.com/chullhwan-song/Reading-Paper/issues/39)
